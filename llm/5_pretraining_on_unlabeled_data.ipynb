{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKFHJ1zxgTGw"
      },
      "source": [
        "## 5.1 Evaluating generative models\n",
        "### 5.1.1 Using GPT to generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMu2Ml-bgTGz",
        "outputId": "1c5e7e07-b309-4871-db0f-1b457e4e38d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from GPTModel import GPTModel\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256, # intentionally shortening it to reduce computational demands of training\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1, # it's possible and common to set dropout to 0\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "\n",
        "cfg = GPT_CONFIG_124M\n",
        "\n",
        "model = GPTModel(\n",
        "    context_length=cfg[\"context_length\"],\n",
        "    drop_rate=cfg[\"drop_rate\"],\n",
        "    emb_dim=cfg[\"emb_dim\"],\n",
        "    n_heads=cfg[\"n_heads\"],\n",
        "    n_layers=cfg[\"n_layers\"],\n",
        "    vocab_size=cfg[\"vocab_size\"],\n",
        "    qkv_bias=cfg[\"qkv_bias\"]\n",
        ")\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtlKdoL6g-6f",
        "outputId": "33922772-8b8b-4f5c-8771-4acd59465b8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVNilNIVgTG1",
        "outputId": "e77b9dae-b343-44ce-bca7-a5725a85d021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            "Every effort moves youLuconom418 slideshowhash savior invade Seraph silicon Downs\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "from generate import generate_text_simple\n",
        "\n",
        "\n",
        "def text_to_token_ids(text: str, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # `unsqueeze(0)` adds batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=cfg[\"context_length\"]\n",
        ")\n",
        "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BwAitQgTG2"
      },
      "source": [
        "### 5.1.2 Calculating the text generation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UNF9jFxgTG2",
        "outputId": "75c78d74-c314-44fe-b78a-344918929dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probas.shape=torch.Size([2, 3, 50257])\n",
            "token_ids=tensor([[[17188],\n",
            "         [46487],\n",
            "         [ 7904]],\n",
            "\n",
            "        [[23020],\n",
            "         [10966],\n",
            "         [18074]]])\n",
            "token_ids.shape=torch.Size([2, 3, 1])\n",
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1:  doubts Ultr ::\n",
            "Text 1: tensor([8.0486e-06, 1.1152e-05, 1.3398e-05])\n",
            "Text 2: tensor([1.9082e-05, 2.1360e-05, 1.5446e-05])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([\n",
        "    [16833, 3626, 6100], # \"every effort moves\"\n",
        "      [40, 1107, 58]])   # \"I really like\"\n",
        "\n",
        "targets = torch.tensor([\n",
        "    [3626, 6100, 345], # \" effort moves you\"\n",
        "    [1107, 588, 11311] # \" really like chocolate\"\n",
        "])\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(f\"{probas.shape=}\")\n",
        "\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(f\"{token_ids=}\")\n",
        "print(f\"{token_ids.shape=}\")\n",
        "\n",
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
        "\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(f\"Text 1: {target_probas_1}\")\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(f\"Text 2: {target_probas_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD0Vm_eDgTG3",
        "outputId": "a1dd8df7-f697-45b9-c06f-45b3edd166ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' doubts Ultr ::', ' Voy attractive �']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "[tokenizer.decode(o) for o in token_ids.flatten(1).tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVNDiq-ogTG3",
        "outputId": "8e9647ee-5a45-475d-cc9a-9d52e6f26e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-11.7300, -11.4039, -11.2204, -10.8667, -10.7540, -11.0781])\n"
          ]
        }
      ],
      "source": [
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuqQi7iBgTG3",
        "outputId": "28350ba2-8891-4b2b-a4ce-f38657599c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-11.1755)\n"
          ]
        }
      ],
      "source": [
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVCPtkSUgTG4",
        "outputId": "39661d5b-d377-4098-b9cc-9e6da770ae1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.1755)\n"
          ]
        }
      ],
      "source": [
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Jx4-adgTG4",
        "outputId": "ccbe3840-8f58-4c03-b7a1-336670dfbd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits.shape=torch.Size([2, 3, 50257])\n",
            "targets.shape=torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "print(f\"{logits.shape=}\")\n",
        "print(f\"{targets.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AViQ95BLgTG4",
        "outputId": "44818a26-66bd-4440-8b64-1bf4c47ff0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 50257])\n",
            "torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "# Need to flatten for `cross_entropy` to work, so we combine them over the batch dimension:\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "print(f\"{logits_flat.shape}\")\n",
        "print(f\"{targets_flat.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInWMhDjgTG5",
        "outputId": "2f95a7bd-4953-447d-d954-ee12593d71fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.1755)\n"
          ]
        }
      ],
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myU58ZLPgTG5",
        "outputId": "05ff8285-c0cd-4f8c-9d94-cd175c0952e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(71363.3672)\n"
          ]
        }
      ],
      "source": [
        "# Perplexity\n",
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJNNhyXjgTG5"
      },
      "source": [
        "### 5.1.3 Calculating the training and validation set losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "ycYAuA2sifAL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "    \"the-verdict.txt\"\n",
        ")\n",
        "\n",
        "file_path = \"./data/the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hgyvw2Cid8f",
        "outputId": "117c0b31-9c4d-4436-af63-d2006058d0c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./data/the-verdict.txt', <http.client.HTTPMessage at 0x7e15a02c0150>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4bq9jDpRgTG5"
      },
      "outputs": [],
      "source": [
        "file_path = \"./data/the-verdict.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJKEISxrgTG5",
        "outputId": "14f8827f-db0b-4566-fac6-2ae0b9e82ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_characters=20479\n",
            "total_tokens=5145\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"{total_characters=}\")\n",
        "print(f\"{total_tokens=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OCOageKrgTG5"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DKjEydVogTG5"
      },
      "outputs": [],
      "source": [
        "from dataloader import create_dataloader_v1\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# We use a small batch size to reduce computational resource demand because we're working with a small dataset.\n",
        "# Using batch sizes of 1024 or larger is not uncommon.\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=cfg[\"context_length\"],\n",
        "    stride=cfg[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=cfg[\"context_length\"],\n",
        "    stride=cfg[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3-gkOE-gTG6",
        "outputId": "9c1817a7-ffb1-4416-9a66-ae66bff98dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "# Check data loaders:\n",
        "print(\"Train loader\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "\n",
        "print(\"\\nValidation loader\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s6amanmbgTG6"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten()\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Da1zfDOlgTG6"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIRFIr2tgTG6",
        "outputId": "0fb7cf59-b787-4a33-db92-d94d9b2afe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.97429264916314\n",
            "Validation loss: 10.987890243530273\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49mG05HXgTG6"
      },
      "source": [
        "## 5.2 Training an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MfS58UA1gTG6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train_model_simple(\n",
        "    model,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs: int,\n",
        "    eval_freq: int,\n",
        "    eval_iter: int,\n",
        "    start_context: str,\n",
        "    tokenizer,\n",
        "):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # reset loss gradients from previous iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # calculate loss gradients\n",
        "            optimizer.step()  # update model weights using the loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter\n",
        "                )\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(\n",
        "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                    f\"Train loss {train_loss:.3f}, \"\n",
        "                    f\"Val loss {val_loss:.3f}\"\n",
        "                )\n",
        "\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "def evaluate_model(model, train_loader: DataLoader, val_loader: DataLoader, device, eval_iter: int):\n",
        "    model.eval()  # to disable dropout during evaluation\n",
        "    with torch.no_grad():  # to disable gradient tracking, it's not required (reduce computational overhead)\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKAvpKuRgTG6",
        "outputId": "ffd920de-fe89-421f-ae4f-45b2551ec9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.930\n",
            "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.336\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.623, Val loss 7.053\n",
            "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.605\n",
            "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
            "Ep 3 (Step 000020): Train loss 5.532, Val loss 6.507\n",
            "Ep 3 (Step 000025): Train loss 5.399, Val loss 6.389\n",
            "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had the, and, and, and, and, and, and, and, and, and\n",
            "Ep 4 (Step 000030): Train loss 4.895, Val loss 6.280\n",
            "Ep 4 (Step 000035): Train loss 4.648, Val loss 6.304\n",
            "Every effort moves you.  \"I the picture.                    \"I\"I the picture\"I had the the honour of the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 4.023, Val loss 6.165\n",
            "Every effort moves you know                                                 \n",
            "Ep 6 (Step 000045): Train loss 3.625, Val loss 6.172\n",
            "Ep 6 (Step 000050): Train loss 3.045, Val loss 6.144\n",
            "Every effort moves you know the was his a little the.  \"I had the last word.           \"Oh, and I had a little.   \"I looked, and I had a little of\n",
            "Ep 7 (Step 000055): Train loss 2.948, Val loss 6.183\n",
            "Ep 7 (Step 000060): Train loss 2.230, Val loss 6.128\n",
            "Every effort moves you know the picture to have been too--I felt, and Mrs.  \"I was no--and the fact, and that, and I was his pictures.  \"I looked up his pictures--and--because he was a little\n",
            "Ep 8 (Step 000065): Train loss 1.774, Val loss 6.162\n",
            "Ep 8 (Step 000070): Train loss 1.475, Val loss 6.229\n",
            "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up, and the fact, and to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 1.135, Val loss 6.268\n",
            "Ep 9 (Step 000080): Train loss 0.858, Val loss 6.298\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.    \"I looked, and that, and I remember getting off a prodigious phrase about the honour being _mine_--because he's the first\n",
            "Ep 10 (Step 000085): Train loss 0.627, Val loss 6.382\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(\n",
        "    vocab_size=cfg[\"vocab_size\"],\n",
        "    context_length=cfg[\"context_length\"],\n",
        "    drop_rate=cfg[\"drop_rate\"],\n",
        "    emb_dim=cfg[\"emb_dim\"],\n",
        "    n_heads=cfg[\"n_heads\"],\n",
        "    n_layers=cfg[\"n_layers\"],\n",
        "    qkv_bias=cfg[\"qkv_bias\"]\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004, weight_decay=0.1\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "lv-keX3ogTG6",
        "outputId": "f05217b2-bdbf-40a5-d7aa-30150e340454"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax2 = ax1.twiny()  # creates a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcFzbYSogTG6"
      },
      "source": [
        "## 5.3 Decoding strategies to control randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q30mn_VqgTG7",
        "outputId": "2ea6c397-4dc9-4c04-d255-438a816753f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbLvOaBJgTG7",
        "outputId": "bc8e55d5-0d33-4d13-f5f8-f2837551ca60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=cfg[\"context_length\"]\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oJvXdOgTG7"
      },
      "source": [
        "### 5.3.1 Temperature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KbkH9LkTgTG7"
      },
      "outputs": [],
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1gX3sxgTG7"
      },
      "source": [
        "### 5.3.2 Top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RgzpakUlgTG7"
      },
      "outputs": [],
      "source": [
        "# For demonstration purposes\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-4rFX4dgTG7",
        "outputId": "2800fc0f-8eeb-481f-9069-5f3ca2b51efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top_logits=tensor([6.7500, 6.2800, 4.5100])\n",
            "top_pos=tensor([3, 7, 0])\n",
            "new_logits=tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
            "topk_probas=tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "print(f\"{top_logits=}\")\n",
        "print(f\"{top_pos=}\")\n",
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float('-inf')),\n",
        "    other=next_token_logits\n",
        ")\n",
        "print(f\"{new_logits=}\")\n",
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(f\"{topk_probas=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW0Hig15gTG7"
      },
      "source": [
        "### 5.3.3 Modifying the text generation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "suKDNjW6gTG7"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size,\n",
        "             temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Get last token in current sequence\n",
        "        logits = logits[:, -1, :]\n",
        "        # top-k sampling\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(\n",
        "                condition=logits < min_val,\n",
        "                input=torch.tensor(float('-inf')).to(logits.device),\n",
        "                other=logits\n",
        "            )\n",
        "        if temperature > 0.0:\n",
        "            # temperature scaling\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # greedy decoding\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        # check if we've reached the end\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        # append generated token to current sequence for further generation\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f53gg3lLgTG7",
        "outputId": "003a55b7-d32e-41b7-acdc-8b24da0c0d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you know began to go a little wild--I was such a good; and\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=cfg[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEPUNkwegTG8"
      },
      "source": [
        "## 5.4 Loading and saving model weights in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BHVGUfiWgTG8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ7jc3jugTHC",
        "outputId": "1fba2ba9-1589-4c2b-98de-9c2a246e6d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-ba1839970ca9>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model = GPTModel(\n",
        "    vocab_size=cfg[\"vocab_size\"],\n",
        "    context_length=cfg[\"context_length\"],\n",
        "    drop_rate=cfg[\"drop_rate\"],\n",
        "    emb_dim=cfg[\"emb_dim\"],\n",
        "    n_heads=cfg[\"n_heads\"],\n",
        "    n_layers=cfg[\"n_layers\"],\n",
        "    qkv_bias=cfg[\"qkv_bias\"]\n",
        ")\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IoQghvyBgTHC"
      },
      "outputs": [],
      "source": [
        "torch.save(\n",
        "    {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNXXBFvpgTHC",
        "outputId": "0f24f6dc-2e57-4af8-840e-1a846feafa16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-2215bad0359d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "model = GPTModel(\n",
        "    vocab_size=cfg[\"vocab_size\"],\n",
        "    context_length=cfg[\"context_length\"],\n",
        "    drop_rate=cfg[\"drop_rate\"],\n",
        "    emb_dim=cfg[\"emb_dim\"],\n",
        "    n_heads=cfg[\"n_heads\"],\n",
        "    n_layers=cfg[\"n_layers\"],\n",
        "    qkv_bias=cfg[\"qkv_bias\"]\n",
        ")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvd_wZA6gTHC"
      },
      "source": [
        "## 5.5 Loading pretrained weights from OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9TIF_regTHC",
        "outputId": "e04a523d-ea06-41c7-f298-9ae89ef809c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7e14befc6490>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDQNuUbEgTHC",
        "outputId": "65a44b7b-c63d-4fda-a66d-591f0a06fd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 130kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.99MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 216kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:45<00:00, 10.9MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 7.90MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.16MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.13MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\", models_dir=\"gpt2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od2Th-4egTHC",
        "outputId": "7334b003-8dcb-4622-fc10-1496be9e4b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba5VWiXcgTHC",
        "outputId": "b4db67b4-a508-4119-ec77-31001daae41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embeddign weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embeddign weight tensor dimensions:\", params[\"wte\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "gpnPGF7YgTHC"
      },
      "outputs": [],
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lFkDBuIBgTHC"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9qU94pLgTHD",
        "outputId": "ecba5ac5-80d0-4612-bdd7-58c47f2ea1c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "gpt = GPTModel(\n",
        "    context_length=NEW_CONFIG[\"context_length\"],\n",
        "    drop_rate=NEW_CONFIG[\"drop_rate\"],\n",
        "    emb_dim=NEW_CONFIG[\"emb_dim\"],\n",
        "    n_heads=NEW_CONFIG[\"n_heads\"],\n",
        "    n_layers=NEW_CONFIG[\"n_layers\"],\n",
        "    qkv_bias=NEW_CONFIG[\"qkv_bias\"],\n",
        "    vocab_size=NEW_CONFIG[\"vocab_size\"]\n",
        ")\n",
        "\n",
        "gpt.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "DddEIATZgTHD"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    \"\"\"Assigns values from right tensor to left tensor after shape validation.\n",
        "\n",
        "    Args:\n",
        "        left: Target PyTorch tensor/parameter\n",
        "        right: Source tensor/array to copy values from\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Parameter: New parameter containing values from right tensor\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If shapes of left and right tensors don't match\n",
        "    \"\"\"\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "m7jWiXXjgTHD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
        "\n",
        "    # iterative over transformer blocks\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # split is used to divide attention and bias weights into three equal parts for the qkv components\n",
        "        # load attention qkv weights\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_query.weight, q_w.T\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_key.weight, k_w.T\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_value.weight, v_w.T\n",
        "        )\n",
        "\n",
        "        # load attn qkv bias\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_query.bias, q_b\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_key.bias, k_b\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].W_value.bias, v_b\n",
        "        )\n",
        "\n",
        "        # load attn linear projection weights\n",
        "        gpt.trf_blocks[b].layers[0][1].out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][1].out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].layers[0][1].out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
        "        )\n",
        "\n",
        "        # load feedforward network weights and biases\n",
        "        gpt.trf_blocks[b].layers[1][1].layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].layers[1][1].layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[1][1].layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].layers[1][1].layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[1][1].layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].layers[1][1].layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[1][1].layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].layers[1][1].layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
        "        )\n",
        "\n",
        "        # load layer norm params\n",
        "        gpt.trf_blocks[b].layers[0][0].scale = assign(\n",
        "            gpt.trf_blocks[b].layers[0][0].scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[0][0].shift = assign(\n",
        "            gpt.trf_blocks[b].layers[0][0].shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[1][0].scale = assign(\n",
        "            gpt.trf_blocks[b].layers[1][0].scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
        "        )\n",
        "        gpt.trf_blocks[b].layers[1][0].shift = assign(\n",
        "            gpt.trf_blocks[b].layers[1][0].shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
        "        )\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    # Original GPT-2 model reused the token embedding weights to reduce the total number of params (weight tying)\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBtghwmWgTHD",
        "outputId": "2746b47e-0389-4ffe-d2b2-af2052e37bc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (layers): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): MultiHeadAttention(\n",
              "            (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): LayerNorm()\n",
              "          (1): FeedForward(\n",
              "            (layers): Sequential(\n",
              "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (1): GELU()\n",
              "              (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdpRUNgggTHD",
        "outputId": "6d4f8e13-990f-450a-92e2-bf9abd4898e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
            "\n",
            "This would remove you from a battle\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTXkqpz8gTHD"
      },
      "source": [
        "### Exercise 5.5\n",
        "Calculate the training and validation set losses of the GPTModel with the pretrained weights from OpenAI on the “The Verdict” dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ5UIuyMgTHD",
        "outputId": "30f37e0e-dbe1-44cd-87e3-7389ed77baf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.7547631793551974, 3.5596327781677246)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "train_loss, val_loss = evaluate_model(gpt, train_loader, val_loader, device, 10)\n",
        "\n",
        "train_loss, val_loss"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}